#Retrieval

##1. Source
##2. Load

- TextLoader
```python
from langchain.document_loaders import TextLoader

loader = TextLoader("example.txt")
docs = loader.load()
```

- CSVLoader
```python
from langchain_community.document_loaders.csv_loader import CSVLoader

loader = CSVLoader(
    ...  # <-- Integration specific parameters here
)
data = loader.load()
```

- PyPDFLoader
```python
from langchain_community.document_loaders.pdf_loader import PyPDFLoader

loader = PyPDFLoader("example.pdf")
docs = loader.load()
```

- UnstructuredFileLoader(파일 형식 상관없이 로드: 텍스트 파일, 파워포인트, html, pdf, 이미지 등)     
```python
from langchain_community.document_loaders.html_loader import UnstructuredFileLoader

loader = UnstructuredFileLoader("example.html")
docs = loader.load()
```

- Document loaders from langchain: https://python.langchain.com/docs/integrations/document_loaders/


##3. Split

사용자의 질문에 대답하는 데에 필요한 부분만을 llm에 전달할 수 있도록 문서를 잘게 쪼개어놓는 작업

- 로드 후 쪼개기(splitter.split_documents)
```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000, 
    chunk_overlap=200, # 문장/문단이 중간에 잘리는 것을 방지하기 위해 사용
    )
docs = text_splitter.split_documents(docs)
```

- 로드하며 쪼개기(loader.load_and_split)
```python
from langchain_community.document_loaders.csv_loader import CSVLoader

splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200,)
loader = CSVLoader(
    ...  # <-- Integration specific parameters here
)
docs = loader.load_and_split(text_splitter=splitter)
```

- 특정 문자열을 기준으로 쪼개기(CharacterTextSplitter)
```python
from langchain.text_splitter import CharacterTextSplitter

text_splitter = CharacterTextSplitter(
    separator="\n", # 줄바꿈을 기준으로 쪼개기
    chunk_size=1000,
    chunk_overlap=200,
)
docs = loader.load_and_split(text_splitter=text_splitter)
```

- 토큰 기준으로 쪼개기(TokenTextSplitter)

(from Cursor)
```python
from langchain.text_splitter import TokenTextSplitter

text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=200)
docs = text_splitter.split_documents(docs)
```

(ticktoken encoder)
```python
from langchain.text_splitter import CharacterTextSplitter

text_splitter = CharacterTextSplitter.from_tiktoken_encoder(
    separator="\n",
    chunk_size=1000, 
    chunk_overlap=200,
)
docs = text_splitter.split_documents(docs)
```

##4. Embed or Vectorize

문서를 벡터로 변환하는 작업

##5. Store

벡터 검색을 위해 데이터 저장

- Vector stores from langchain: https://python.langchain.com/docs/integrations/vectorstores/

```python
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings # 대표적인 embedding 모델

embedding = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(
    docs, #split된 문서
    embedding,
)
```

- 벡터 검색(vectorstore.similarity_search)

질문과 유사한 내용의 문서 조각을 찾아줌

```python
vectorstore.similarity_search(query)
```

- 캐싱: 요금 절감을 위해 embedding 결과를 캐싱하여 사용

```python
from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings


cache_dir = LocalFileStore(
   "./.cache", #gitignore 파일에 추가
)

embedding = OpenAIEmbeddings()

cache_embeddings = CacheBackedEmbeddings.from_bytes_store(
    embedding,
    cache_dir,
)

vectorstore = Chroma.from_documents(
    docs,
    cache_embeddings,
)
```


##6. Retrieve






