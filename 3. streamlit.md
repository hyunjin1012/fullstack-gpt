#실행

streamlit run home.py

## home.py 변경 후 자동 재실행

streamlit run home.py --server.runOnSave true

# 사이드바

``` python
st.title("Chat with your PDF")

with st.sidebar:
    st.title("Sidebar")
    st.write("This is a sidebar")
```

# 본문 탭

``` python
st.title("Chat with your PDF")

tab1, tab2 = st.tabs(["Tab 1", "Tab 2"])

with tab1:
    st.write("This is tab 1")
```

# 브라우저 탭

``` python
st.set_page_config(
    page_title="Chat with your PDF", 
    page_icon=":books:",
    layout="wide",
    initial_sidebar_state="expanded",
)
```

# 페이지 추가

1. Pages 폴더 생성
2. 폴더 내에 파일 생성
3. 파일 이름 앞에 숫자 추가
4. 파일 이름 순서대로 페이지 추가

``` python
st.write("This is page 1")
```

# 채팅 메시지

``` python
with st.chat_message("user"):
    st.write("Hello, how are you?")
with st.chat_message("assistant"):
    st.write("I'm fine, thank you!")

st.chat_input("Ask me anything!")
```

# 상태 표시

``` python
with st.status("Loading...", expanded=True) as status:
   time.sleep(2)
   st.write("Getting ready...")
   time.sleep(2)
   st.write("Ready!")
   status.update(label="Done", state="complete")
```

# session_state
코드 재실행(화면 상 변경사항 발생시 파이썬은 항상 코드 재실행) 시에도 기완료된 채팅 메시지가 사라지지 않도록 사용

``` python
if "messages" not in st.session_state: 
    st.session_state["messages"] = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("Ask me anything!"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    response = chat_with_pdf(prompt)
    st.session_state.messages.append({"role": "assistant", "content": response})
```

# .env 읽기

니코는 아래처럼 하라고 했지만 그냥 .env 파일도 잘 읽히는 듯...?

1. .streamlit 폴더 생성
2. .streamlit 폴더 내에 .secrets.toml 파일 생성
3. .secrets.toml 파일 내에 키-값 쌍 추가
4. 서버 재시작
5. .gitignore 파일 내에 .streamlit 폴더 추가

# @st.cache_data

함수 내에서 캐시 데이터를 사용하여 함수 실행 속도를 향상시키는 데 사용

``` python
@st.cache_data(show_spinner=True) #파일 최초 업로드시에만 스피너가 표시되며 이후 채팅창에 변화가 생겨도 스피너가 표시되지 않음
def embed_file(file):
    file_content = file.read()
    file_path = f"./.cache/files/{file.name}"
   ...
    return retriever 

if file:
    retriever = embed_file(file) #해당 파일이 캐시에 존재할 경우 캐시에서 불러오고, 없을 경우 함수 실행
```

# 스트리밍

``` python

class ChatCallbackHandler(BaseCallbackHandler):
    message = ""
    def on_llm_start(self, *args, **kwargs):
        self.message_box = st.empty()
    def on_llm_end(self, *args, **kwargs):
        save_message(self.message, "ai")
    def on_llm_new_token(self, token, *args, **kwargs):
        self.message += token
        self.message_box.markdown(self.message)

llm = ChatOpenAI(
    temperature=0.1,
    streaming=True,
    callbacks=[
        ChatCallbackHandler(),
    ],

    ...

     if message:
        send_message(message, "human")
        chain = (
            {
                "context": retriever | RunnableLambda(format_docs),
                "question": RunnablePassthrough(),
            }
            | prompt
            | llm
        )
        with st.chat_message("ai"):
            response = chain.invoke(message)
else:
    st.session_state["messages"] = []
```

